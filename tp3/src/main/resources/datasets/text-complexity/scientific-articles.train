The new study reveals that mass extinctions among some groups of amniotes coincide with numerous and large diversifications in other closely related groups.

Conducted by scientists from the Museum für Naturkunde in Berlin, Germany, and the University of Lincoln, UK, the research challenges commonly held views that support a relationship between the evolution of "key innovations" in a group and the rapid increase in its number of species. The researchers behind the new study suggest the evidence for such a relationship has only ever been circumstantial.

The new study examined the issue of adaptive radiations among early amniotes, from 315 to 200 million years ago. This time period witnessed some of the most profound climate changes on a global scale, including the dramatic shrinking of the southern polar icecap, the disappearance of equatorial rainforests, a substantial increase in temperature, and prolonged drought conditions. The time period under study also included the largest mass extinction in Earth's history, about 252 million years ago.

The concept of adaptive radiation is central to modern evolutionary biology. An adaptive radiation is an extremely rapid increase in the number of species in a group, often as a result of a key evolutionary innovation, which gives the group an advantage over its competitors or allows it to exploit a new resource. Often, if the appearance of an evolutionary novelty coincides temporarily with a large increase in species richness, it is assumed that the innovation is responsible for this pattern.

The scientists used statistical methods to identify which of the amniote groups present during this time were significantly more species-rich than their close relatives, and attempted to identify the factors responsible for this diversity imbalance. The results suggest that, usually, large differences in diversity between two closely related groups are not because more species evolve in the larger group, but rather because more species of the smaller group go extinct.

A key finding of the research is that even the appearance of an important innovation in the larger group does not trigger a large proliferation of species until a major new extinction takes place.

Dr Neil Brocklehurst, a postdoc at the Museum für Naturkunde in Berlin, is the lead author of the paper. He said: "It appears that these 'key innovations' do not promote massive increases in species richness, but instead buffer against extinction when times get tough."

As part of their study, the scientists examined the evolution of the dicynodont therapsids, a group of extinct plant-eaters closely related to mammals. About 270 million years ago, dicynodonts evolved a toothless beak and a back-and-forwards motion of the lower jaw, all of which are clear functional adaptations to help them process plant material. However, it was not until 10 million years later, during a minor extinction event, that dicynodonts began to outcompete their close relatives and became hugely diverse.

A similar pattern is seen in sauropodomorph dinosaurs, the group which would go on to produce the largest land vertebrates of all times, such as the celebrated Brachiosaurus housed at the Museum für Naturkunde Berlin. The large-bodied members of this group do end up becoming significantly more diverse than their close relatives, but not until a mass extinction event that took place at the end of the Triassic, almost 30 million years after their first appearance.

Co-author Dr Marcello Ruta, Senior Lecturer in the School of Life Sciences at the University of Lincoln, UK, explained: "Using large-scale tree diagrams to depict the evolutionary relationships of various groups of organisms, it is possible to address major evolutionary questions that Charles Darwin, and many generations of biologists after him, asked. How did life become so diverse? What triggers major diversification events? How do animals and plants respond to global catastrophes?"

Co-author Jörg Fröbisch, Professor for Palaeobiology and Evolution at the Museum für Naturkunde and Humboldt-Universität zu Berlin, added: "Surprisingly, when these early terrestrial vertebrates evolved a novel structure or function, they did not undergo a dramatic increase in species number. Instead, an adaptive radiation usually occurs much later in the history of these groups, during one of the many extinction events or during times of climate stress."

Co-author Johannes Müller, Professor of Palaeozoology at the Museum für Naturkunde and Humboldt-Universität zu Berlin, said: "We really did not expect any of these patterns. Our results go against many of the traditional predictions from evolutionary biology, and show that the scientific views about the relevance of key innovations should be carefully reconsidered."
The global average surface temperature in 2015 is likely to be the warmest on record and to reach the symbolic and significant milestone of 1° Celsius above the pre-industrial era. This is due to a combination of a strong El Niño and human-induced global warming, according to the World Meteorological Organization (WMO).

The years 2011-2015 have been the warmest five-year period on record, with many extreme weather events -- especially heatwaves -- influenced by climate change, according to a WMO five-year analysis.

"The state of the global climate in 2015 will make history as for a number of reasons," said WMO Secretary-General Michel Jarraud. "Levels of greenhouse gases in the atmosphere reached new highs and in the Northern hemisphere spring 2015 the three-month global average concentration of CO2 crossed the 400 parts per million barrier for the first time. 2015 is likely to be the hottest year on record, with ocean surface temperatures at the highest level since measurements began. It is probable that the 1°C Celsius threshold will be crossed," said Mr Jarraud. "This is all bad news for the planet."

Greenhouse gas emissions, which are causing climate change, can be controlled. We have the knowledge and the tools to act. We have a choice. Future generations will not."

"Added to that, we are witnessing a powerful El Niño event, which is still gaining in strength. This is influencing weather patterns in many parts of the world and fuelled an exceptionally warm October. The overall warming impact of this El Niño is expected to continue into 2016," said Mr Jarraud.

WMO issued its provisional statement on the status of the climate in 2015, and an additional five-year analysis for 2011-2015, to inform negotiations at the U.N. Climate Change Conference in Paris.

A preliminary estimate based on data from January to October shows that the global average surface temperature for 2015 so far was around 0.73 °C above the 1961-1990 average of 14.0°C and approximately 1°C above the pre-industrial 1880-1899 period.

This temperature tendency indicates that 2015 will very likely be the warmest year on record. The global average sea-surface temperature, which set a record last year, is likely to equal or surpass that record in 2015. The global average temperatures over land areas only from January to October suggest that 2015 is also set to be one of the warmest years on record over land. South America is having its hottest year on record, as is Asia (similar to 2007), and Africa and Europe their second hottest.

According to preliminary figures as of the end of September 2015, 2011-15 was the world's warmest five-year period on record, at about 0.57°C (1.01°F) above the average for the standard 1961-90 reference period. It was the warmest five-year period on record for Asia, Europe, South America and Oceania, and for North America. WMO compiled the five-year analysis because it provides a longer-term climate signal than the annual report.

Highlights of 2015:

El Niño:

The full effect of the strong 2015 El Niño on global temperature is likely to continue after El Niño peaks. However, other impacts are already being felt. In early October, NOAA declared that record global ocean temperatures had led to a global coral bleaching event. This began in the North Pacific in the summer of 2014 and spread to the South Pacific and Indian Ocean in 2015.

Consistent with typical El Niño impacts, large areas of Central America and the Caribbean recorded below average rainfall. Brazil, which started the year in drought in southern and eastern areas, saw the focus of the drought shift north with scant rainfall during the dry season over the Amazon. India's monsoon rainfall was 86% of normal. In Indonesia, the low rainfall has likely contributed to the increased incidence of wildfires. Peru was affected by heavy rain and flooding, as was Argentina.

Ocean heat and sea level rise

The oceans have been absorbing more than 90% of the energy that has accumulated in the climate system from human emissions of greenhouse gases, resulting in higher temperatures and sea levels. In the first nine months of 2015, global ocean heat content through both the upper 700 meters and 2000 meters of the oceans reached record high levels. The latest estimates of global sea level indicate that the global average sea level in the first half of 2015 was the highest since satellite observations became available in 1993.

Significant warmth was recorded across large areas of the oceans. The Tropical Pacific was much warmer than average, exceeding 1°C over much of the central and eastern equatorial Pacific, consistent with the signature of a strong El Niño. The northeast Pacific, much of the Indian Ocean and areas in the north and south Atlantic were significantly warmer than average. Areas to the south of Greenland and in the far southwest Atlantic were significantly colder than average.

Regional temperatures

Significant warmer than average temperatures were recorded over the majority of observed land areas, especially western North America, large areas of South America, Africa and southern and eastern Eurasia. China had its warmest January-to-October period on record. For the continent of Africa, 2015 currently ranks as the second warmest year on record. Australia had its warmest October on record and a heatwave early in the month set new records for early season warmth.

One notably cold area was the Antarctic, where a strong anomaly in atmospheric patterns known as the Southern Annular Mode lasted for several months. Eastern areas of north America were colder than average during the year, but none were record cold. After a warm January to September, Argentina experienced its coldest October on record.

Heatwaves

A major heatwave affected India in May and June, with average maximum temperatures exceeded 42°C widely and 45°C in some areas. In southern Pakistan temperatures exceeded 40 °C in June.

Heatwaves affected Europe, northern Africa and the Middle East through the late spring and summer, with many new temperature records set. In May, high temperatures affected Burkina Faso, Niger and Morocco. Spain and Portugal also saw unusually high temperatures. July brought heat waves to a large area from Denmark in the north, to Morocco in the south and Iran in the east. In early August, Jordan experienced a heatwave, whilst Wroclaw (Poland) experienced an all-time high temperature of 38.9°C on the 8th August. The heat continued into September, shifting further into Eastern Europe.

During the spring of 2015 in South Africa, record high temperatures were exceeded on a regular basis.

Rainfall and drought

Areas of high rainfall included: southern areas of the USA, Mexico, Bolivia, southern Brazil, southeast Europe, areas of Pakistan and Afghanistan. Heavy rain in January led to flooding in Malawi, Zimbabwe and Mozambique, and in February it affected Morocco, Algeria and Tunisia. 2015 saw exceptional seasonal rainfall totals in several parts of Burkina Faso and Mali.

March in Chile saw unusually heavy rains which caused flooding and mudslides. In August, heavy rain in the Buenos Aires province of Argentina saw several monthly and daily rainfall records broken during the month. Mexico had its wettest March on record (since 1941). It was the wettest May on record for the contiguous USA and the wettest month overall in 121 years of record keeping. Between May and October, China experienced 35 heavy rain events. Subsequent flooding affected 75 million people with estimated economic losses of 25 billion dollars.

Long-term rainfall patterns can disguise great variability in short-term totals. There were many instances in 2015 of 24-hour totals exceeding the normal monthly mean. For instance, the Moroccan city of Marrakech received 35.9mm of rain in one hour in August, over 13 times the monthly normal. In Pakistan during the monsoon, one station recorded 540mm of rain in 24 hours; the annual normal is 336mm.

Dry areas included Central America and the Caribbean, northeast South America including Brazil, parts of central Europe and Russia, parts of Southeast Asia, Indonesia and southern Africa. In Western North America, long-term drought conditions continued. Basins across the west depend on snowpack as a water resource. On April 1, the snow water equivalent was 5% of normal.

The dry and warm conditions observed across much of the western USA during the year favoured the development of wildfires. In Alaska, over 400 fires burned 728,000 hectares in May, breaking the previous record of 216 fires and 445,000 hectares. Over 700 wildfires were reported in Alaska during July, burning nearly 2 million hectares during the summer. Large fires burned throughout the Northwest in August and Washington State suffered its largest fire on record.

Tropical Cyclones

Globally, a total of 84 tropical storms formed between the start of the year and 10 November, compared to the 1981-2010 annual average of 85. Hurricane Patricia which made landfall in Mexico on 24 October was the strongest hurricane on record in either the Atlantic or eastern North Pacific basins, with maximum sustained wind speeds of 320 km/hour. In the Northwest Pacific basin, 25 named storms were recorded. Six typhoons made landfall over China, with three leading to combined estimated economic losses of 8 billion dollars.

Four named storms formed in the Northern Indian Ocean. Rainfall associated with tropical storm Komen contributed to severe flooding and landslides in Myanmar. Bangladesh also suffered from flash floods and landslides. Yemen suffered from unprecedented back-to-back cyclones in early November, with Chapala becoming first tropical cyclone to make landfall, followed by Megh.

The South Pacific saw 9 named storms. Tropical cyclone Pam made landfall over Vanuatu as a category 5 cyclone on 13 March destroying many homes.

Arctic and Antarctic

Since consistent satellite records began in the late 1970s, there has been a general decline in Arctic sea ice extent throughout the seasonal cycle. In 2015, the daily maximum extent, which occurred on 25th February 2015, was the lowest on record at 14.54 million km2. The minimum sea ice extent was on 11th September when the extent was 4.41 million km2, the fourth lowest in the satellite record.

In the southern hemisphere, the daily maximum extent of 18.83 million km2 was recorded on 6th October in Antarctica. This is the 16th highest maximum extent in the satellite record. The minimum extent, recorded on 20 February, was 3.58 million km2, the 4th highest on record.

Climate Change Attribution

Scientific assessments have found that many extreme events in the 2011-15 period, especially those relating to extreme high temperatures, have had their probabilities over a particular time period substantially increased as a result of human-induced climate change -- by a factor of 10 or more in some cases.

Of 79 studies published by Bulletin of the American Meteorological Society between 2011 and 2014, more than half found that anthropogenic climate change contributed to extreme events. The most consistent influence has been on extreme heat, with some studies finding that the probability of the observed event has increased by 10 times or more.

Examples include the record high seasonal and annual temperatures in the United States in 2012 and in Australia in 2013, hot summers in eastern Asia and western Europe in 2013, heatwaves in spring and autumn 2014 in Australia, record annual warmth in Europe in 2014, and the Argentine heatwave of December 2013.

Some longer-term events, which have not yet been the subject of formal attribution studies, are consistent with projections of near- and long-term climate change. These include increased incidence of multi-year drought in the subtropics, as manifested in the 2011-15 period in the southern United States, parts of southern Australia and, towards the end of the period, southern Africa. There have also been events, such as the unusually prolonged, intense and hot dry seasons in the Amazon basin of Brazil in both 2014 and 2015 which, while they cannot yet be stated with confidence to be part of a long-term trend, are of considerable concern in the context of potential "tipping points" in the climate system as identified by the Intergovernmental Panel on Climate Change.
Was it a catastrophic collision in the star's asteroid belt? A giant impact that disrupted a nearby planet? A dusty cloud of rock and debris? A family of comets breaking apart? Or was it alien megastructures built to harvest the star's energy?

Just what caused the mysterious dimming of star KIC 8462852?

Massimo Marengo, an Iowa State University associate professor of physics and astronomy, wondered when he saw all the buzz about the mysterious star found by citizen scientists on the Planet Hunters website.

Those citizen scientists were highlighting measurements of star brightness recorded by NASA's Kepler spacecraft. Tiny dips in a star's brightness can indicate a planet is passing in front of the star. That's how Kepler astronomers -- and citizen scientists using the internet to help analyze the light curves of stars -- are looking for planets.

But this star had deep dips in brightness -- up to 22 percent. The star's brightness also changed irregularly, sometimes for days and even months at a time. A search of the 150,000-plus stars in Kepler's database found nothing like this.

So Marengo and two other astronomers decided to take a close look at the star using data taken with the Infrared Array Camera of NASA's Spitzer Space Telescope. They report their findings in a paper recently published online by The Astrophysical Journal Letters.

Their conclusion?

"The scenario in which the dimming in the KIC 8462852 light curve were caused by the destruction of a family of comets remains the preferred explanation …," wrote the three -- Marengo; Alan Hulsebus, an Iowa State doctoral student; and Sarah Willis, a former Iowa State graduate student now with the Massachusetts Institute of Technology's Lincoln Laboratory.

Questions about the star were launched last month when a research team led by Tabetha Boyajian of Yale University reported on the star in the Monthly Notices of the Royal Astronomical Society. The astronomers reported how citizen scientists tagged the star's deep and irregular dips in brightness as "bizarre" and "interesting."

Boyajian and the other researchers looked at the data and investigated several possible causes. They wrote the "most promising theory" was a barrage of crumbling comets passing in front of the star.

In a subsequent paper submitted to The Astrophysical Journal, Jason Wright and colleagues at Penn State University speculated about other causes, including alien megastructures built to harvest energy while orbiting the star.

When the Iowa State astronomers studied the star with Spitzer infrared data from January 2015 -- two years after the Kepler measurements -- Marengo said they didn't see much. If there had been some kind of catastrophe near the star, he said there would be a lot of dust and debris. And that would show up as extra infrared emissions.

Marengo said the study looked at two different infrared wavelengths: the shorter was consistent with a typical star and the longer showed some infrared emissions, but not enough to reach a detection threshold. The astronomers concluded there were no excess infrared emissions and therefore no sign of an asteroid belt collision, a giant impact on a planet or a dusty cloud of rock and debris.

So Marengo and his colleagues say the destruction of a family of comets near the star is the most likely explanation for the mysterious dimming. The comet fragments coming in rapidly at a steep, elliptical orbit could create a big debris cloud that could dim the star. Then the cloud would move off, restoring the star's brightness and leaving no trace of excess infrared light.

And the alien megastructure theory?

"We didn't look for that," Marengo said. "We can't really say it is, or is not. But what the star is doing is very strange. It's interesting when you have phenomena like that -- typically it means there's some new physical explanation or a new concept to be discovered."

In new research appearing in the journal Nature Biotechnology, an international research team led by The Hebrew University of Jerusalem describes a new technique for growing human hepatocytes in the laboratory. This groundbreaking development could help advance a variety of liver-related research and applications, from studying drug toxicity to creating bio-artificial liver support for patients awaiting transplantations.

The liver is the largest internal organ in the human body, serving as the main site of metabolism. Human hepatocytes -- cells that comprise 85% of the liver -- are routinely used by the pharmaceutical industry for study of hepatotoxicity, drug clearance and drug-drug interactions. They also have clinical applications in cell therapy to correct genetic defects, reverse cirrhosis, or support patients with a liver-assist device.

Regrettably, while the human liver can rapidly regenerate in vivo, recognized by the ancient Greeks in the myth of Prometheus, this capability to proliferate is rapidly lost when human cells are removed from the body. Thus far, attempts to expand human hepatocytes in the laboratory resulted in immortalized cancer cells with little metabolic function. The scarce supply of human hepatocytes and this inability to expand them without losing function is a major bottleneck for scientific, clinical and pharmaceutical development.

To address this problem, Prof. Yaakov Nahmias, director of the Alexander Grass Center for Bioengineering at the Hebrew University of Jerusalem, partnered with leading German scientists at upcyte technologies GmbH (formerly Medicyte) to develop a new approach to rapidly expand the number of human liver cells in the laboratory without losing their unique metabolic function.

Based on early work emerging from the German Cancer Research Center (DKFZ) on the Human Papilloma Virus (HPV), the research team demonstrated that weak expression of HPV E6 and E7 proteins released hepatocytes from cell-cycle arrest and allowed them to proliferate in response to Oncostatin M (OSM), a member of the interleukin 6 (IL-6) superfamily that is involved in liver regeneration. Whereas previous studies caused hepatocytes to proliferate without control, turning hepatocytes into tumor cells with little metabolic function, the researchers carefully selected colonies of human hepatocytes that only proliferate in response to OSM. Stimulation with OSM caused cell proliferation, with doubling time of 33 to 49 hours. Removal of OSM caused growth arrest and hepatic differentiation within 4 days, generating highly functional cells. The method, described as the upcyte© process (upcyte technologies GmbH), allows expanding human hepatocytes for 35 population doubling, resulting in 1015 cells (quadrillion) from each liver isolation. By comparison, only 109 cells (billion) can be isolated from a healthy organ.

"The approach is revolutionary," said Dr. Joris Braspenning, who led the German group. "Its strength lies in our ability to generate liver cells from multiple donors, enabling the study of patient-to-patient variability and idiosyncratic toxicity." The team generated hepatocyte lines from ethnically diverse backgrounds that could be serially passaged, while maintaining CYP450 activity, epithelial polarization, and protein expression at the same level as primary human hepatocytes. Importantly, the proliferating hepatocytes showed identical toxicology response to primary human hepatocytes across 23 different drugs.

"This is the holy grail of liver research," said Prof. Nahmias, the study's lead author. "Our technology will enable thousands of laboratories to study fatty liver disease, viral hepatitis, drug toxicity and liver cancer at a fraction of the current cost." Nahmias noted that genetic modifications preclude using the cells for transplantation, "but we may have found the perfect cell source for the bio-artificial liver project."

The proliferating hepatocyte library was recently commercialized by upcyte technologies GmbH (Hamburg, Germany), which is expanding the scope of the technology. "upcyte© hepatocytes represent the next generation of cell technology," said Dr. Astrid Nörenberg, the company's managing director. "We are poised to become the leading cell supplier for pharmaceutical development and chemical toxicity testing."

By collecting samples from the portal vein--which carries blood from the gastrointestinal tract, including from the pancreas, to the liver--physicians can learn far more about a patient's pancreatic cancer than by relying on peripheral blood from a more easily accessed vein in the arm.

Primary tumors shed cancerous cells, known as circulating tumor cells (CTCs), into the blood. These have been widely studied as prognostic biomarkers for various cancers. Because these cells are often larger, irregularly shaped and tend to cluster together, they get trapped in smaller vessels.

The authors hypothesized that most cells released from a gastrointestinal tumor would flow into the portal vein and then get sequestered by the narrow vessels in the liver. These cells would not reach the peripheral venous system. CTCs from gastrointestinal tumors are rarely identified in the peripheral blood until the cancer is widely metastatic.

To test this theory, researchers from the University of Chicago used an ultrasound-guided endoscope and a small needle to take blood from the portal vein during routine diagnostic endoscopies. They found CTCs in 100 percent of 18 patients with suspected tumors in the pancreas and bile ducts. Tests using peripheral blood samples, the standard method, detected tumors cells in only 4 of the 18 patients.

"We demonstrated that this method is potentially quite valuable as well as noninvasive, feasible and safe," said study director Irving Waxman, MD, professor of medicine and surgery and director of the Center for Endoscopic Research and Therapeutics at the University. "We had no complications related to portal vein blood acquisition."

The findings could offer doctors a method to diagnose pancreatic cancer earlier in patients. Only seven percent of patients diagnosed with stage II disease are still alive five years after diagnosis, making it one of the most lethal forms of cancer. The American Cancer Society estimates that in 2015, nearly 49,000 people will be diagnosed with pancreatic cancer and 40,560 people with this disease will die.

The portal vein samples contained far more tumor cells in all stages evaluated, including locally advanced as well as metastatic tumors, the researchers report online in the journal Gastroenterology. Blood collected from the portal vein had a mean of more than 100 CTCs per 7.5 milliliters. Patients with less advanced disease, who could potentially benefit from surgery to remove the tumor, had fewer CTCs. Those patients averaged about 80 CTCs per 7.5 milliliters.

In contrast, when the researchers used peripheral blood to test the same patients, they found few, if any, circulating tumor cells. Those samples contained, on average, less than one CTC in 7.5 milliliters of blood, the equivalent of one cell in a billion.

"Access to circulating tumor cells may help us define the diagnosis and guide treatment," Waxman said. "Having the ability to count them and to probe their molecular profiles can make a substantial difference in how we treat each patient's tumor."

"In the setting of localized cancer where these findings are most applicable, the additional information of portal vein CTC number and their molecular characterization may help predict who will benefit from aggressive therapy before surgery, who is most at risk for a recurrence after the operation, and even who will not benefit from surgery at all," said the study's co-first author, Daniel Catenacci, MD, assistant professor of medicine at the University of Chicago.

These hidden cells in the portal venous system could help cancer specialists make better clinical decisions. Molecular characterization of CTCs at the time of diagnosis or after neoadjuvant therapy can provide clues about each patient's prognosis. The frequent loss of protective tumor-suppressor genes--such as TP53, SMAD4 and p16/CDK2NA, which are often inactivated in pancreatic cancer--correlates with a worse outcome.

"This is a novel and far more sensitive way to acquire, enumerate, and characterize CTCs from pancreatobiliary and other gastrointestinal cancers in this setting," Waxman said. "We believe it can improve how we stratify patients."

Patients who don't have CTCs in the portal vein, for example, should have a better prognosis than those who do. Treatments can be personalized accordingly.

The authors agree that more studies need to be done to confirm their hypothesis with larger numbers of patients in a controlled setting. They plan further collaborative efforts.

"Ultimately, we envision that this new test could help plan treatment, based on a much more accurate record of the number and characteristics of circulating tumor cells," said co-first author Christopher Chapman, MD, a member of the Center for Endoscopic Research and Therapeutics at the University of Chicago. "That should allow us to make better, more informed judgements about prognosis, and avoid interventions, such as surgery, that might not help."

When a high-fat diet causes us to become obese, it also appears to prompt normally bustling immune cells in our brain to become sedentary and start consuming the connections between our neurons, scientists say.

The good news is going back on a low-fat diet for just two months, at least in mice, reverses this trend of shrinking cognitive ability as weight begins to normalize, said Dr. Alexis M. Stranahan, neuroscientist in the Department of Neuroscience and Regenerative Medicine at the Medical College of Georgia.

"Microglia eating synapses is contributing to synapse loss and cognitive impairment in obesity," Stranahan said. "On the one hand, that is very scary, but it's also reversible, meaning that if you go back on a low-fat diet that does not even completely wipe out the adiposity, you can completely reverse these cellular processes in the brain and maintain cognition."

Stranahan is corresponding author of the study in the journal Brain, Behavior, and Immunity, which provides some of the first evidence of why fat is bad for the brain.

The trouble appears to start with too much fat in the body producing chronic inflammation, which stimulates microglia to have an autoimmune response. Microglia, like macrophages in the body, are known for their ability to ingest trash and infectious agents in the brain, and their highly acidic interior gets rids of it, which helps support the function and health of neurons. But as mice get obese, their microglia seem focused on overeating.

"Normally in the brain, microglia are constantly moving around. They are always moving around their little fingers and processes. What happens in obesity is they stop moving," Stranahan said. "They draw in all their processes; they basically just sit there and start eating synapses. When microglia start eating synapses, the mice don't learn as effectively," Stranahan said.

The study looked at normal male mice: One group ate a diet in which about 10 percent of the calories came from saturated fat, and another consumed chow that was 60 percent fat. To ensure other factors were equal, the researchers chose chows that had similar levels of other key ingredients such as macronutrients and protein. The chows were on par with a healthy diet versus a fast-food diet in humans. "If you look at the lipid breakdown for the two diets, these guys are getting crazy, crazy amounts," Stranahan said of the high-fat-fare mice.

At four, eight and 12 weeks, the MCG scientists took a series of metabolic measures, such as weight, food intake, insulin and serum glucose levels. They also measured in the hippocampus, the center of learning and memory, levels of synaptic markers, proteins found at synapses that correlate with the number of synapses.

"This gives us a window into what is occurring at the level of the synapse and also microglial activation," Stranahan said. And, they measured levels of inflammatory cytokines, which microglia produce when "they start getting activated and angry."

All levels in both groups were essentially the same at four weeks. The mice on a high-fat diet were fatter, but other measures were normal at eight weeks. By 12 weeks the fat-eating mice were obese, had elevated cytokine levels and a reduction in the markers for synapse number and function.

"When you get out to 12 weeks, you start seeing great increases in peripheral obesity. While you don't see insulin resistance, you also start seeing loss of synapses and increases in inflammatory cytokines in the brain," Stranahan said.

At that point, the research team switched half the mice on the high-fat diet to the low-fat regimen. It took about two months for their weight to return to normal, although their overall fat pad remained larger than their peers who had never gained weight. That fat layer makes it easier to gain weight in the future, Stranahan notes. As with most people, the mice that remained on the low-fat diet slowly accumulated a little weight as they aged.

Meanwhile, the group that stayed on the high-fat diet kept getting fatter, more inflamed and losing synapses, she said. Their microglia's little processes, or protrusions, which normally help monitor synaptic function and help these cells move, continued to wither. Dendritic spines on neurons, which get input from synapses, similarly withered on the high-fat diet, but like the microglia processes, were restored with the lower-fat fare.

"That is very promising," said Stranahan. The findings also point to some potential new purposes for existing drugs now used for conditions such as rheumatoid arthritis and Crohn's disease, which block specific inflammatory cytokines and tumor necrosis factor alpha, both of which are elevated in the brains of the fat mice.

Obesity yields extreme overkill in microglia, which are typically extremely discriminating and helpful to neurons. During development, for example, they will prune a synapse that isn't functioning. "That is one way the developing brain refines itself. It allows you to keep only those synapses that you need or the synapses you have been using. Fat dramatically alters their dynamic.

"Instead of doing garbage disposal, they are taking your mailbox, your front door, your kitchen sink and all the stuff that you need, and not doing their job of getting rid of trash," Stranahan said.

She notes that the high-fat-eating mice actually ate less chow and consumed the same amount of calories as mice eating low fat. "The entire metabolic phenotype is driven by diet composition rather than the amount of calories," Stranahan said. If high-fat-eating mice had greater variety in their diet, such as a sugar-water option, they might also consume more total calories, similar to the sensory-specific satiety phenomenon in humans, she said.

New Johns Hopkins Bloomberg School of Public Health-led research finds that blood taken from children up to the age of five contains molecular evidence about whether their mothers smoked during pregnancy.

The findings, published online in the journal Environmental Research, offer strong evidence that environmental exposures that go as far back as the womb may continue to remain in the body and potentially affect someone's health for years after birth. They also suggest that with further research it could be possible to detect exposures to other potential toxins during pregnancy that are less evident such as to chemicals in plastics, undetected infections or contaminants in drinking water.

Ultimately, the hope would be to link these exposures to chronic diseases such as autism, obesity or heart disease to better understand how diseases develop and possibly help prevent them.

"If you have a blood sample, you may be able to ask research questions that you could never ask before," says study leader M. Daniele Fallin, PhD, the Sylvia and Harold Halpert Professor and Chair of the Bloomberg School's Department of Mental Health. "Smoking is one thing. But if this turns out to be possible for other kinds of exposures, this could be a paradigm shift.

"We have long known that the body is an accumulator of past exposures -- evidence of lead exposure lives on in our bones, for example. But we did not know that something as easy to collect as blood could contain evidence of exposures not only during your life but prenatally. That's what makes this so compelling."

For this new proof of principle study, Fallin and her colleagues did an analysis of epigenetics, molecules that are not part of the DNA sequence but sit on top of it and regulate which genes are turned on and off when and where in the body. Two years ago, another group of researchers looked at newborn cord blood and found that the amount of an epigenetic mark, known as DNA methylation, at 26 locations on the genome, was correlated with whether that baby's mother had smoked during pregnancy.

For this study, Fallin and her colleagues took the experiment a step further. They tested the blood of 531 preschoolers from six different sites in the United States and also spoke to their mothers about whether or not they smoked during pregnancy. They again analyzed methylation patterns at the same 26 locations in the genome and found that 81 percent of the time their test was able to accurately predict prenatal smoking exposure. It was not previously known whether this epigenetic signature would still be around as many as five years later, but the blood still contained this molecular memory.

It is possible, the researchers say, that the signature is also related to exposure to secondhand smoke after birth, but that would not account for all of it since at birth -- before they could be exposed to secondhand smoke -- those whose mothers smoked while pregnant already had the signature.

Fallin says she hopes this area of research has broader reach. With smoking, she says, it is relatively easy to determine whether someone was exposed to cigarette smoking in the womb: You simply ask the mother or ask someone whether their mother smoked while she was pregnant. But exposures to other toxins are more difficult to tease out. For many, the mother may not know whether she was exposed.

"If epigenetic signatures can be found for other environmental exposures, these could provide clues to how certain prenatal exposures affect health and potentially decades into life," says co-author Christine Ladd-Acosta, PhD, an assistant professor of epidemiology at the Bloomberg School.

Fallin, who also directs the Bloomberg School's Wendy Klag Center for Autism & Developmental Disabilities, says she is specifically hoping to determine whether in utero exposures are related to the development of autism. She says it remains unclear whether these epigenetic biomarkers are a direct cause of chronic diseases later in life or what else they may indicate.

A nugget of real 20 carats gold, so light that it does not sink in a cappuccino, floating instead on the milk foam -- what sounds unbelievable has actually been accomplished by researchers from ETH Zurich. Scientists led by Raffaele Mezzenga, Professor of Food and Soft Materials, have produced a new kind of foam out of gold, a three-dimensional mesh of gold that consists mostly of pores. It is the lightest gold nugget ever created. "The so-called aerogel is a thousand times lighter than conventional gold alloys. It is lighter than water and almost as light as air," says Mezzenga.

The new gold form can hardly be differentiated from conventional gold with the naked eye -- the aerogel even has a metallic shine. But in contrast to its conventional form, it is soft and malleable by hand. It consists of 98 parts air and only two parts of solid material. Of this solid material, more than four-fifths are gold and less than one-fifth is milk protein fibrils. This corresponds to around 20 carat gold.

Drying process a challenge

The scientists created the porous material by first heating milk proteins to produce nanometre-fine protein fibres, so-called amyloid fibrils, which they then placed in a solution of gold salt. The protein fibres interlaced themselves into a basic structure along which the gold simultaneously crystallised into small particles. This resulted in a gel-like gold fibre network.

"One of the big challenges was how to dry this fine network without destroying it," explains Gustav Nyström, postdoc in Mezzenga's group and first author of the corresponding study in the journal Advanced Materials. As air drying could damage the fine gold structure, the scientists opted for a gentle and laborious drying process using carbon dioxide. They did so in an interdisciplinary effort assisted by researchers in the group of Marco Mazzotti, Professor of Process Engineering.

Dark-red gold

The method chosen, in which the gold particles are crystallised directly during manufacture of the aerogel protein structure (and not, for example, added to an existing scaffold) is new. The method's biggest advantage is that it makes it easy to obtain a homogeneous gold aerogel, perfectly mimicking gold alloys.

The manufacturing technique also offers scientists numerous possibilities to deliberately influence the properties of gold in a simple manner. " The optical properties of gold depend strongly on the size and shape of the gold particles," says Nyström. "Therefore we can even change the colour of the material. When we change the reaction conditions in order that the gold doesn't crystallise into microparticles but rather smaller nanoparticles, it results in a dark-red gold." By this means, the scientists can influence not only the colour, but also other optical properties such as absorption and reflection.

The new material could be used in many of the applications where gold is currently being used, says Mezzenga. The substance's properties, including its lighter weight, smaller material requirement and porous structure, have their advantages. Applications in watches and jewellery are only one possibility. Another application demonstrated by the scientists is chemical catalysis: since the highly porous material has a huge surface, chemical reactions that depend on the presence of gold can be run in a very efficient manner. The material could also be used in applications where light is absorbed or reflected. Finally, the scientists have also shown how it becomes possible to manufacture pressure sensors with it. "At normal atmospheric pressure the individual gold particles in the material do not touch, and the gold aerogel does not conduct electricity," explains Mezzenga. "But when the pressure is increased, the material gets compressed and the particles begin to touch, making the material conductive."

An international team of researchers has predicted the existence of a new type of particle called the type-II Weyl fermion in metallic materials. When subjected to a magnetic field, the materials containing the particle act as insulators for current applied in some directions and as conductors for current applied in other directions. This behavior suggests a range of potential applications, from low-energy devices to efficient transistors.

The researchers theorize that the particle exists in a material known as tungsten ditelluride (WTe2), which the researchers liken to a "material universe" because it contains several particles, some of which exist under normal conditions in our universe and others that may exist only in these specialized types of crystals. The research appeared in the journal Nature this week.

The new particle is a cousin of the Weyl fermion, one of the particles in standard quantum field theory. However, the type-II particle exhibits very different responses to electromagnetic fields, being a near perfect conductor in some directions of the field and an insulator in others.

The research was led by Princeton University Associate Professor of Physics B. Andrei Bernevig, as well as Matthias Troyer and Alexey Soluyanov of ETH Zurich, and Xi Dai of the Chinese Academy of Sciences Institute of Physics. The team included Postdoctoral Research Associates Zhijun Wang at Princeton and QuanSheng Wu at ETH Zurich, and graduate student Dominik Gresch at ETH Zurich.

The particle's existence was missed by physicist Hermann Weyl during the initial development of quantum theory 85 years ago, say the researchers, because it violated a fundamental rule, called Lorentz symmetry, that does not apply in the materials where the new type of fermion arises.

Particles in our universe are described by relativistic quantum field theory, which combines quantum mechanics with Einstein's theory of relativity. Under this theory, solids are formed of atoms that consist of a nuclei surrounded by electrons. Because of the sheer number of electrons interacting with each other, it is not possible to solve exactly the problem of many-electron motion in solids using quantum mechanical theory.

Instead, our current knowledge of materials is derived from a simplified perspective where electrons in solids are described in terms of special non-interacting particles, called quasiparticles, that move in the effective field created by charged entities called ions and electrons. These quasiparticles, dubbed Bloch electrons, are also fermions.

Just as electrons are elementary particles in our universe, Bloch electrons can be considered the elementary particles of a solid. In other words, the crystal itself becomes a "universe," with its own elementary particles.

In recent years, researchers have discovered that such a "material universe" can host all other particles of relativistic quantum field theory. Three of these quasiparticles, the Dirac, Majorana, and Weyl fermions, were discovered in such materials, despite the fact that the latter two had long been elusive in experiments, opening the path to simulate certain predictions of quantum field theory in relatively inexpensive and small-scale experiments carried out in these "condensed matter" crystals.

These crystals can be grown in the laboratory, so experiments can be done to look for the newly predicted fermion in WTe2 and another candidate material, molybdenum ditelluride (MoTe2).

"One's imagination can go further and wonder whether particles that are unknown to relativistic quantum field theory can arise in condensed matter," said Bernevig. There is reason to believe they can, according to the researchers.

The universe described by quantum field theory is subject to the stringent constraint of a certain rule-set, or symmetry, known as Lorentz symmetry, which is characteristic of high-energy particles. However, Lorentz symmetry does not apply in condensed matter because typical electron velocities in solids are very small compared to the speed of light, making condensed matter physics an inherently low-energy theory.

"One may wonder," Soluyanov said, "if it is possible that some material universes host non-relativistic 'elementary' particles that are not Lorentz-symmetric?"

This question was answered positively by the work of the international collaboration. The work started when Soluyanov and Dai were visiting Bernevig in Princeton in November 2014 and the discussion turned to strange unexpected behavior of certain metals in magnetic fields (Nature 514, 205-208, 2014, doi:10.1038/nature13763). This behavior had already been observed by experimentalists in some materials, but more work is needed to confirm it is linked to the new particle.

The researchers found that while relativistic theory only allows a single species of Weyl fermions to exist, in condensed matter solids two physically distinct Weyl fermions are possible. The standard type-I Weyl fermion has only two possible states in which it can reside at zero energy, similar to the states of an electron which can be either spin-up or spin-down. As such, the density of states at zero energy is zero, and the fermion is immune to many interesting thermodynamic effects. This Weyl fermion exists in relativistic field theory, and is the only one allowed if Lorentz invariance is preserved.

The newly predicted type-2 Weyl fermion has a thermodynamic number of states in which it can reside at zero energy -- it has what is called a Fermi surface. Its Fermi surface is exotic, in that it appears along with touching points between electron and hole pockets. This endows the new fermion with a scale, a finite density of states, which breaks Lorentz symmetry.

The discovery opens many new directions. Most normal metals exhibit an increase in resistivity when subject to magnetic fields, a known effect used in many current technologies. The recent prediction and experimental realization of standard type-I Weyl fermions in semimetals by two groups in Princeton and one group in IOP Beijing showed that the resistivity can actually decrease if the electric field is applied in the same direction as the magnetic field, an effect called negative longitudinal magnetoresistance. The new work shows that materials hosting a type-II Weyl fermion have mixed behavior: While for some directions of magnetic fields the resistivity increases just like in normal metals, for other directions of the fields, the resistivity can decrease like in the Weyl semimetals, offering possible technological applications.

"Even more intriguing is the perspective of finding more 'elementary' particles in other condensed matter systems," the researchers say. "What kind of other particles can be hidden in the infinite variety of material universes? The large variety of emergent fermions in these materials has only begun to be unraveled."

Researchers at Princeton University were supported by the U.S. Department of Defense, the U.S. Office of Naval Research, the U.S. National Science Foundation, the David and Lucile Packard Foundation and the W.M. Keck Foundation. Researchers at ETH Zurich were supported by Microsoft Research, the Swiss National Science Foundation and the European Research Council. Xi Dai was supported by the National Natural Science Foundation of China, the 973 program of China and the Chinese Academy of Sciences.

A new computer algorithm can predict whether you and your spouse will have an improved or worsened relationship based on the tone of voice that you use when speaking to each other with nearly 79 percent accuracy.

In fact, the algorithm did a better job of predicting marital success of couples with serious marital issues than descriptions of the therapy sessions provided by relationship experts. The research was published in Proceedings of Interspeech on September 6, 2015.

Researchers recorded hundreds of conversations from over one hundred couples taken during marriage therapy sessions over two years, and then tracked their marital status for five years.

An interdisciplinary team -- led by Shrikanth Narayanan and Panayiotis Georgiou of the USC Viterbi School of Engineering with their doctoral student Md Nasir and collaborator Brian Baucom of University of Utah -- then developed an algorithm that broke the recordings into acoustic features using speech-processing techniques. These included pitch, intensity, "jitter" and "shimmer" among many -- things like tracking warbles in the voice that can indicate moments of high emotion.

"What you say is not the only thing that matters, it's very important how you say it. Our study confirms that it holds for a couple's relationship as well," Nasir said. Taken together, the vocal acoustic features offered the team's program a proxy for the subject's communicative state, and the changes to that state over the course of a single therapy and across therapy sessions.

These features weren't analyzed in isolation -- rather, the impact of one partner upon the other over multiple therapy sessions was studied.

"It's not just about studying your emotions," Narayanan said. "It's about studying the impact of what your partner says on your emotions."

"Looking at one instance of a couple's behavior limits our observational power," Georgiou said. "However, looking at multiple points in time and looking at both the individuals and the dynamics of the dyad can help identify trajectories of the their relationship."

Once it was fine-tuned, the program was then tested against behavioral analyses made by human experts ‹ who had coded them for positive qualities like "acceptance" or negative qualities like ³blame." The team found that studying voice directly -- rather than the expert-created behavioral codes -- offered a more accurate glimpse at a couple's future.

"Psychological practitioners and researchers have long known that the way that partners talk about and discuss problems has important implications for the health of their relationships. However, the lack of efficient and reliable tools for measuring the important elements in those conversations has been a major impediment in their widespread clinical use. These findings represent a major step forward in making objective measurement of behavior practical and feasible for couple therapists," Baucom said.

Next, using behavioral signal processing -- a framework developed by Narayanan for computationally understanding human behavior -- the team plans to use language (e.g., spoken words) and nonverbal information (e.g., body language) to improve the prediction of how effective treatments will be.

Researchers in the Cockrell School of Engineering at The University of Texas at Austin have developed a first-of-its-kind self-healing gel that repairs and connects electronic circuits, creating opportunities to advance the development of flexible electronics, biosensors and batteries as energy storage devices.

Although technology is moving toward lighter, flexible, foldable and rollable electronics, the existing circuits that power them are not built to flex freely and repeatedly self-repair cracks or breaks that can happen from normal wear and tear.

Until now, self-healing materials have relied on application of external stimuli such as light or heat to activate repair. The UT Austin "supergel" material has high conductivity (the degree to which a material conducts electricity) and strong mechanical and electrical self-healing properties.

"In the last decade, the self-healing concept has been popularized by people working on different applications, but this is the first time it has been done without external stimuli," said mechanical engineering assistant professor Guihua Yu, who developed the gel. "There's no need for heat or light to fix the crack or break in a circuit or battery, which is often required by previously developed self-healing materials."

Yu and his team created the self-healing gel by combining two gels: a self-assembling metal-ligand gel that provides self-healing properties and a polymer hydrogel that is a conductor. A paper on the synthesis of their hydrogel appears in the November issue of Nano Letters.

In this latest paper, the researchers describe how they used a disc-shaped liquid crystal molecule to enhance the conductivity, biocompatibility and permeability of their polymer hydrogel. They were able to achieve about 10 times the conductivity of other polymer hydrogels used in bioelectronics and conventional rechargeable batteries. The nanostructures that make up the gel are the smallest structures capable of providing efficient charge and energy transport.

In a separate paper published in Nano Letters in September, Yu introduced the self-healing hybrid gel. The second ingredient of the self-healing hybrid gel is a metal-ligand supramolecular gel. Using terpyridine molecules to create the framework and zinc atoms as a structural glue, the molecules form structures that are able to self-assemble, giving it the ability to automatically heal after a break.

When the supramolecular gel is introduced into the polymer hydrogel, forming the hybrid gel, its mechanical strength and elasticity are enhanced.

To construct the self-healing electronic circuit, Yu believes the self-healing gel would not replace the typical metal conductors that transport electricity, but it could be used as a soft joint, joining other parts of the circuit.

"This gel can be applied at the circuit's junction points because that's often where you see the breakage," he said. "One day, you could glue or paste the gel to these junctions so that the circuits could be more robust and harder to break."

Yu's team is also looking into other applications, including medical applications and energy storage, where it holds tremendous potential to be used within batteries to better store electrical charge.

Yu's research has received funding from the National Science Foundation, the American Chemical Society, the Welch Foundation and 3M.

A new study from the University of Exeter, published in the journal Ecology Letters, found that phytoplankton -- microscopic water-borne plants -- can rapidly evolve tolerance to elevated water temperatures. Globally, phytoplankton absorb as much carbon dioxide as tropical rainforests and so understanding the way they respond to a warming climate is crucial.

Phytoplankton subjected to warmed water initially failed to thrive but it took only 45 days, or 100 generations, for them to evolve tolerance to temperatures expected by the end of the century. With their newfound tolerance came an increase in the efficiency in which they were able to convert carbon dioxide into new biomass.

The results show that evolutionary responses in phytoplankton to warming can be rapid and might offset some of the predicted declines in the ability of aquatic ecosystems to absorb carbon dioxide as the planet warms.

Dan Padfield a PhD student at the Environment and Sustainability Institute at the University of Exeter's Penryn Campus in Cornwall said: "Our findings suggest that evolution could play a key role in shaping how aquatic ecosystems respond to climate change. The phytoplankton in our study adapted to warmer water in the lab and evolved the ability to capture more atmospheric carbon dioxide.

"Our results demonstrate that evolutionary responses of phytoplankton to warming should be taken into account when developing models of how climate change will affect aquatic ecosystems. This experimental work provides the empirical basis for incorporating evolution into the models used to forecast future ocean productivity."

The researchers exposed Chlorella vulgaris, a model species of phytoplankton, to temperatures of 20 -- 33 degrees. Initially rates of growth peaked at 30 degrees, while 33 degrees was stressful and limited growth. After 100 generations (45 days) growth increased to levels expected from the exponential effects of temperature on physiological rates, showing that the algae had evolved the ability to thrive at the increased temperatures.

The underlying mechanism for the ability to tolerate warmer temperatures was an increase in the efficiency in which the alga was able to convert carbon dioxide into new biomass by reducing rates of respiration (production of carbon dioxide). It is this shift in the relative rates of respiration and photosynthesis that enabled the phytoplankton to cope with warmer temperatures.

While these experiments focused on a single species and strain of phytoplankton, the researchers believe that the rapid evolution of carbon-use efficiency will apply to other species of phytoplankton and substantially improve models describing ecological and biogeochemical effects of climate change.

A microscopic marine alga is thriving in the North Atlantic to an extent that defies scientific predictions, suggesting swift environmental change as a result of increased carbon dioxide in the ocean, a study led a by Johns Hopkins University scientist has found.

What these findings mean remains to be seen, however, as does whether the rapid growth in the tiny plankton's population is good or bad news for the planet.

Published Thursday in the journal Science, the study details a tenfold increase in the abundance of single-cell coccolithophores between 1965 and 2010, and a particularly sharp spike since the late 1990s in the population of these pale-shelled floating phytoplankton.

"Something strange is happening here, and it's happening much more quickly than we thought it should," said Anand Gnanadesikan, associate professor in the Morton K. Blaustein Department of Earth and Planetary Sciences at Johns Hopkins and one of the study's five authors.

Gnanadesikan said the Science report certainly is good news for creatures that eat coccolithophores, but it's not clear what those are. "What is worrisome," he said, "is that our result points out how little we know about how complex ecosystems function." The result highlights the possibility of rapid ecosystem change, suggesting that prevalent models of how these systems respond to climate change may be too conservative, he said.

The team's analysis of Continuous Plankton Recorder survey data from the North Atlantic Ocean and North Sea since the mid-1960s suggests rising carbon dioxide in the ocean is causing the coccolithophore population spike, said Sara Rivero-Calle, a Johns Hopkins doctoral student and lead author of the study. A stack of laboratory studies supports the hypothesis, she said. Carbon dioxide is a greenhouse gas already fingered by scientific consensus as one of the triggers of global warming.

"Our statistical analyses on field data from the CPR point to carbon dioxide as the best predictor of the increase" in coccolithophores, Rivero-Calle said. "The consequences of releasing tons of CO2 over the years are already here and this is just the tip of the iceberg."

The CPR survey is a continuing study of plankton, floating organisms that form a vital part of the marine food chain. The project was launched by a British marine biologist in the North Atlantic and North Sea in the early 1930s. It is conducted by commercial ships trailing mechanical plankton-gathering contraptions through the water as they sail their regular routes.

William M. Balch of the Bigelow Laboratory for Ocean Sciences in Maine, a co-author of the study, said scientists might have expected that ocean acidity due to higher carbon dioxide would suppress these chalk-shelled organisms. It didn't. On the other hand, their increasing abundance is consistent with a history as a marker of environmental change.

"Coccolithophores have been typically more abundant during Earth's warm interglacial and high CO2 periods," said Balch, an authority on the algae. "The results presented here are consistent with this and may portend, like the 'canary in the coal mine,' where we are headed climatologically."

Coccolithophores are single-cell algae that cloak themselves in a distinctive cluster of pale disks made of calcium carbonate, or chalk. They play a role in cycling calcium carbonate, a factor in atmospheric carbon dioxide levels. In the short term they make it more difficult to remove carbon dioxide from the atmosphere, but in the long term -- tens and hundreds of thousands of years -- they help remove carbon dioxide from the atmosphere and oceans and confine it in the deep ocean.

In vast numbers and over eons, coccolithophores have left their mark on the planet, helping to show significant environmental shifts. The White Cliffs of Dover are white because of massive deposits of coccolithophores. But closer examination shows the white deposits interrupted by slender, dark bands of flint, a product of organisms that have glassy shells made of silicon, Gnanadesikan said.

"These clearly represent major shifts in ecosystem type," Gnanadesikan said. "But unless we understand what drives coccolithophore abundance, we can't understand what is driving such shifts. Is it carbon dioxide?"

The study was supported by the Sir Alister Hardy Foundation for Ocean Science, which now runs the CPR, and by the Johns Hopkins Applied Physics Laboratory. Other co-authors are Carlos del Castillo, a former biological oceanographer at APL who now leads NASA's Ocean Ecology Laboratory, and Seth Guikema, a former Johns Hopkins faculty member now at the University of Michigan.

During upheaval in Libya in 2013, a window of opportunity opened for scientists from the University of Kansas to perform research at the Zallah Oasis, a promising site for unearthing fossils from the Oligocene period, roughly 30 million years ago.

From that work, the KU-led team last week published a description of a previously unknown anthropoid primate -- a forerunner of today's monkeys, apes and humans -- in the Journal of Human Evolution. They've dubbed their new find Apidium zuetina.

Significantly, it's the first example of Apidium to be found outside of Egypt.

"Apidium is interesting because it was the first early anthropoid primate ever to be found and described, in 1908," said K. Christopher Beard, Distinguished Foundation Professor of Ecology and Evolutionary Biology and senior curator with KU's Biodiversity Institute, who headed the research. "The oldest known Apidium fossils are about 31 million years old, while the youngest are 29 million. Before our discovery in Libya, only three species of Apidium were ever recovered in Egypt. People had come up with the idea that these primates had evolved locally in Egypt."

Beard said evidence that Apidium had dispersed across North Africa was the key facet of the find. He believes shifting climatic and environmental conditions shaped the distribution of species of Apidium, which affected their evolution.

"We've found evidence that climate change -- not warming, but cooling and drying -- across the Eocene-Oligocene boundary probably is the root cause in kicking anthropoid evolution into overdrive," he said. "All of these anthropoids, which were our distant relatives, were living up in the trees -- none of them were coming down. When the world became cooler and dryer in this period, what was previously a continuous belt of forest became more fragmented. This created barriers to gene flow and movement of animals from one part of forest to what used to be adjacent forest."

With a forest broken up, there was an inhibition of gene flow that through time resulted in speciation, or the creation of new species, according to the KU researcher.

"Animals that are sequestered become different species over millions of years," Beard said. "As the climate oscillates again, you've got different species of Apidium. As forests expand and contract, now you've got competition between species of Apidium that have never seen each other before. One species outcompetes the other, the other goes extinct, and we think that's what we're picking up with this Libyan Apidium, which is related to the youngest and largest species of Apidium known from Egypt."

Beard said that Apidium zuetina would have been physically similar to modern-day squirrel monkeys from South America, but with smaller brains, and would have dined on fruits, nuts and seeds.

"We know that Apidium was a very active arboreal monkey, a really good leaper," he said. "We know they actually had fused lower-leg bones just above the ankle joint. That's really unusual for anthropoid primates, and the only reason for it to happen is because you like to jump a lot, as it stabilized the join between those bones and the ankle."

The team identified Apidium zuetina through detailed analysis of its teeth.

"All of the fossils we have so far are just teeth, not even jaw bones -- but fortunately, the teeth of these anthropoids are so distinct and diagnostic that they function like fingerprints at a crime scene," Beard said. "Studying details of cusps and crests on teeth, we can determine evolutionary relationships. It might sound like thin evidence, but I suspect even with whole skeletons we'd still be focused on teeth to determine relationships. This is because teeth evolve rapidly in response to shifting diets, while an animal's skull and skeleton typically evolves more slowly. Fortunately for paleontologists, teeth are well-documented in the fossil record because tooth enamel is the hardest part of a mammal body, durable and easy to fossilize."

Yet, the researchers chose to name Apidium zuetina not after any of its physical characteristics, but after the Zuetina Oil Company that made the dangerous Libyan fieldwork possible.

"Without their logistical support, we couldn't have done this work at all," Beard said. "We did this just after end of the Libyan civil war that led to the overthrow of Gadhafi."

Beard said the discovery took place during a brief lull in violence in Libya. But the trip to the Zallah Oasis was precarious nonetheless.

"We knew it was risky, but we thought we could go because of our local collaborator, Mustafa Salem, a geology professor at Tripoli University," he said. "He's revered as a father figure among Libyan geologists. An oil facility was close to some interesting sites, and after Mustafa contacted a former student who was working there, they provided our team with charter flights to an airstrip near the oil facility. Without that alone, we couldn't have done our fieldwork -- the roads are too dangerous with bandits and the like. They also gave us lodging, food, water and security."

Beard said armed guards accompanied the team everywhere, manning trucks mounted with antiaircraft guns.
What is Science?
false is not amenable to scientifi c investigation. Explanations
that cannot be based on empirical evidence are not a part of sci-
ence (National Academy of Sciences, 1998).
Science is, however, a human endeavor and is subject to
personal prejudices, misapprehensions, and bias. Over time,
however, repeated reproduction and verifi cation of observations
and experimental results can overcome these weaknesses. That
is one of the strengths of the scientifi c process.
Scientifi c knowledge is based on some assumptions (after
Nickels, 1998), such as
• The world is REAL; it exists apart from our sensory per-
ception of it.
• Humans can accurately perceive and attempt to under-
stand the physical universe.
• Natural processes are suffi cient to explain or account
for natural phenomena or events. In other words, scien-
tists must explain the natural in terms of the natural (and
not the supernatural, which, lacking any independent
evidence, is not falsifi able and therefore not science),
although humans may not currently recognize what those
processes are.
• By the nature of human mental processing, rooted in
previous experiences, our perceptions may be inaccu-
rate or biased.
• Scientifi c explanations are limited. Scientifi c knowledge
is necessarily contingent knowledge rather than abso-
lute, and therefore must be evaluated and assessed, and
is subject to modifi cation in light of new evidence. It is
impossible to know if we have thought of every possible
alternative explanation or every variable, and technology
may be limited.
• Scientifi c explanations are probabilistic. The statistical
view of nature is evident implicitly or explicitly when
stating scientifi c predictions of phenomena or explaining
the likelihood of events in actual situations.
As stated in the National Science Education Standards for
the Nature of Science:
Science is a methodical approach to studying the natural
world. Science asks basic questions, such as how does the world
work? How did the world come to be? What was the world like
in the past, what is it like now, and what will it be like in the
future? These questions are answered using observation, test-
ing, and interpretation through logic.
Most scientists would not say that science leads to an
understanding of the truth. Science is a determination of what is
most likely to be correct at the current time with the evidence at
our disposal. Scientifi c explanations can be inferred from con-
fi rmable data only, and observations and experiments must be
reproducible and verifi able by other individuals. In other words,
good science is based on information that can be measured or
seen and verifi ed by other scientists.
The scientifi c method, it could be said, is a way of learning
or a process of using comparative critical thinking. Things that
are not testable or falsifi able in some scientifi c or mathematical
way, now or in the future, are not considered science. Falsifi -
ability is the principle that a proposition or theory cannot be sci-
entifi c if it does not admit the possibility of being shown false.
Science takes the whole universe and any and all phenomena in
the natural world under its purview, limited only by what is fea-
sible to study given our current physical and fi scal limitations.
Anything that cannot be observed or measured or shown to be
Scientists formulate and test their explanations of nature using
observation, experiments, and theoretical and mathematical
models. Although all scientifi c ideas are tentative and subject
to change and improvement in principle, for most major ideas
in science, there is much experimental and observational con-
fi rmation. Those ideas are not likely to change greatly in the
future. Scientists do and have changed their ideas about nature
when they encounter new experimental evidence that does not
match their existing explanations. (NSES, 1996, p. 171)
Layers rocks making up the walls of the Grand Canyon.
1The Nature of Science and the Scientifi c Method
The Standards for Science Teacher Preparation correctly
state that
Understanding of the nature of science—the goals, values and
assumptions inherent in the development and interpretation of
scientifi c knowledge (Lederman, 1992)—has been an objective
of science instruction since at least the turn of the last century.
It is regarded in contemporary documents as a fundamental
attribute of science literacy and a defense against unquestioning
acceptance of pseudoscience and of reported research. Knowl-
edge of the nature of science can enable individuals to make
more informed decisions with respect to scientifi cally based
issues; promote students’ in-depth understandings of “tradi-
tional” science subject matter; and help them distinguish sci-
ence from other ways of knowing...
Research clearly shows most students and teachers do not
adequately understand the nature of science. For example,
most teachers and students believe that all scientifi c investiga-
tions adhere to an identical set of steps known as the scientifi c
method, and that theories are simply immature laws. Even when
teachers understand and support the need to include the nature
of science in their instruction, they do not always do so. Instead
they may rely upon the false assumption that doing inquiry leads
to understanding of science. Explicit instruction is needed both
to prepare teachers and to lead students to understand the nature
of science. (NSTA, 2003, and references therein, p. 16)
Scientifi c Method
Throughout the past millennium, there has been a real-
ization by leading thinkers that the acquisition of knowledge
can be performed in such a way as to minimize inconsistent
conclusions. Rene Descartes established the framework of the
scientifi c method in 1619, and his fi rst step is seen as a guiding
principle for many in the fi eld of science today:
...never to accept anything for true which I did not clearly know
to be such; that is to say, carefully to avoid precipitancy and
prejudice, and to compromise nothing more in my judgment
than what was presented to my mind so clearly and distinctly
as to exclude all ground of methodic doubt. (Discours de la
Méthode, 1637, section I, 120)
By sticking to certain accepted “rules of reasoning,” scien-
tifi c method helps to minimize infl uence on results by personal,
social, or unreasonable infl uences. Thus, science is seen as a
pathway to study phenomena in the world, based upon repro-
ducibly testable and verifi able evidence. This pathway may take
different forms; in fact, creative fl exibility is essential to scien-
tifi c thinking, so there is no single method that all scientists use,
but each must ultimately have a conclusion that is testable and
falsifi able; otherwise, it is not science.
The scientifi c method in actuality isn’t a set sequence of
procedures that must happen, although it is sometimes pre-
sented as such. Some descriptions actually list and number
three to fourteen procedural steps. No matter how many steps
it has or what they cover, the scientifi c method does contain
2
elements that are applicable to most experimental sciences,
such as physics and chemistry, and is taught to students to aid
their understanding of science.
That being said, it is most important that students realize
that the scientifi c method is a form of critical thinking that will
be subjected to review and independent duplication in order to
reduce the degree of uncertainty. The scientifi c method may
include some or all of the following “steps” in one form or
another: observation, defi ning a question or problem, research
(planning, evaluating current evidence), forming a hypothesis,
prediction from the hypothesis (deductive reasoning), experi-
mentation (testing the hypothesis), evaluation and analysis,
peer review and evaluation, and publication.
Observation
The fi rst process in the scientifi c method involves the
observation of a phenomenon, event, or “problem.” The dis-
covery of such a phenomenon may occur due to an interest on
the observer’s part, a suggestion or assignment, or it may be
an annoyance that one wishes to resolve. The discovery may
even be by chance, although it is likely the observer would be
in the right frame of mind to make the observation. It is said
that as a boy, Albert Einstein wanted to know what it would be
like to ride a light beam, and this curious desire stuck with him
throughout his education and eventually led to his incredible
theories of electromagnetism.
Question
Observation leads to a question that needs to be answered
to satisfy human curiosity about the observation, such as why or
how this event happened or what it is like (as in the light beam).
In order to develop this question, observation may involve tak-
ing measures to quantify it in order to better describe it. Scien-
tifi c questions need to be answerable and lead to the formation
of a hypothesis about the problem.
Hypothesis
To answer a question, a hypothesis will be formed. This is
an educated guess regarding the question’s answer. Educated
is highlighted because no good hypothesis can be developed
without research into the problem. Hypothesis development
depends upon a careful characterization of the subject of the
investigation. Literature on the subject must be researched,
which is made all the easier these days by the Internet (although
sources must be verifi ed; preferably, a library data base should
be used). Sometimes numerous working hypotheses may be
used for a single subject, as long as research indicates they are
all applicable. Hypotheses are generally consistent with exist-
ing knowledge and are conducive to further inquiry.
A scientifi c hypothesis has to be testable and also has to be
falsifi able. In other words, there must be a way to try to make3
The Nature of Science and the Scientifi c Method
The Pineal Gland and the “Melatonin
Hypothesis,” 1959–1974, from public fi le
“Profi les in Science, National Library of
Medicine.”
the hypothesis fail. Science is often more about proving a sci-
entifi c statement wrong rather than right. If it does fail, another
hypothesis may be tested, usually one that has taken into con-
sideration the fact that the last tested hypothesis failed.
One fascinating aspect is that hypotheses may fail at one
time but be proven correct at a later date (usually with more
advanced technology). For example, Alfred Wegener’s idea that
the continents have drifted apart from each other was deemed
impossible because of what was known in the early 1900s about
the composition of the continental crust and the oceanic crust.
Geophysics indicated the brittle, lighter continents could not drift
or be pushed through dense ocean crust. Years later, it was shown
that one aspect of Wegener’s idea, that the continents were once
together, was most likely correct (although not as separate units
but as part of a larger plate). These plates didn’t, however, have to
plow through ocean crust. Instead, magma appears to have arisen
between them and formed new oceanic crust while the plates car-
rying the continents diverged on either side The exact mechanism
of how the plates were pushed apart from the rising magma, or
were pulled apart, allowing magma to rise between them, or a
combination of both, is still not completely understood.
The hypothesis should also contain a prediction about
its verifi ability. For example, if the hypothesis is true,
then (1) should happen when (2) is manipulated.
The fi rst blank (1) is the dependent variable (it depends
on what you are doing in the second blank) and the second
blank (2) is the independent variable (you manipulate it to get
a reaction). There should be no other variables in the experi-
ment that may affect the dependent variable.
One thing is clear about the requirement of the testability
of hypotheses: it must exclude supernatural explanations. If the
supernatural is defi ned as events or phenomena that cannot be
perceived by natural or empirical senses, then they do not fol-
low any natural rules or regularities and so cannot be scientifi -
cally tested. It would be diffi cult to test the speed of angels or
the density of ghosts when they are not available in the natural
world for scientifi c testing, although certainly people have tried
to determine if such entities are real and testable, and it cannot
be precluded that someday technology may exist that can test
certain “supernatural” phenomenon.
Experiment
Once the hypothesis has been established, it is time to test
it. The process of experimentation is what sets science apart
from other disciplines, and it leads to discoveries every day.
An experiment is designed to prove or disprove the hypoth-
esis. If your prediction is correct, you will not be able to reject
the hypothesis.
The average layperson may think of the above kind of pic-
ture when thinking of science experiments. This may be true
in some disciplines, but not all. Einstein relied on mathematics
to “predict” his hypotheses on the nature of space and time in
the universe. His hypotheses had specifi c physical predictionsThe Nature of Science and the Scientifi c Method
4
about space-time, which were shown to be accurate sometimes
years later with developing technology.
Testing and experimentation can occur in the laboratory, in
the fi eld, on the blackboard, or the computer. Results of testing
must be reproducible and verifi able. The data should be avail-
able to determine if the interpretations are unbiased and free
from prejudice.
As the National Science Education Standards state: journals, and in truth, many scientifi c papers submitted to
peer-reviewed journals are rejected. The evaluation process in
science truly makes it necessary for scientists to be accurate,
innovative, and comprehensive.
To better understand the nature of scientifi c laws or theo-
ries, make sure students understand the following defi nitions.
In areas where active research is being pursued and in which
there is not a great deal of experimental or observational evi-
dence and understanding, it is normal for scientists to differ with
one another about the interpretation of the evidence or theory
being considered. Different scientists might publish confl icting
experimental results or might draw different conclusions from
the same data. Ideally, scientists acknowledge such confl ict and
work towards fi nding evidence that will resolve their disagree-
ment. (NSES, 1996, p. 171) Fact: 1. A confi rmed or agreed-upon empirical observa-
tion or conclusion. 2. Knowledge or information based on real
occurrences: an account based on fact. 3. a. Something demon-
strated to exist or known to have existed: Genetic engineering
is now a fact. That Einstein was a real person is an undisputed
fact. b. A real occurrence; an event.
Hypothesis: An educated proposal to explain certain facts;
a tentative explanation for an observation, phenomenon, or sci-
entifi c problem that can be tested by further investigation.
Scientifi c Theory (or Law): An integrated, comprehen-
sive explanation of many “facts,” especially one that has been
repeatedly tested or is widely accepted and can be used to make
predictions about natural phenomena. A theory can often gener-
ate additional hypotheses and testable predictions. Theories can
incorporate facts and laws and tested hypotheses.
Unfortunately, the common/non-scientifi c defi nition for
theory is quite different, and is more typically thought of as a
belief that can guide behavior. Some examples: “His speech
was based on the theory that people hear only what they want
to know” or “It’s just a theory.” Because of the nature of this
defi nition, some people wrongly assume scientifi c theories are
speculative, unsupported, or easily cast aside, which is very far
from the truth. A scientifi c hypothesis that survives extensive
experimental testing without being shown to be false becomes a
scientifi c theory. Accepted scientifi c theories also produce test-
able predictions that are successful.
It is interesting that other scientists may start their own
research and enter the process of one scientist’s work at any
stage. They might formulate their own hypothesis, or they might
adopt the original hypothesis and deduce their own predictions.
Often, experiments are not done by the person who made the
prediction, and the characterization is based on investigations
done by someone else. Published results can also serve as a
hypothesis predicting the reproducibility of those results.
Evaluation
All evidence and conclusions must be analyzed to make
sure bias or inadequate effort did not lead to incorrect conclu-
sions. Qualitative and quantitative mathematical analysis may
also be applied. Scientifi c explanations should always be made
public, either in print or presented at scientifi c meetings. It
should also be maintained that scientifi c explanations are tenta-
tive and subject to modifi cation.
Again, the National Science Education Standards state:
Defi nitions
It is part of scientifi c inquiry to evaluate the results of scientifi c
investigations, experiments, observations, theoretical models,
and the explanations proposed by other scientists. Evaluation
includes reviewing the experimental procedures, examining the
evidence, identifying faulty reasoning, pointing out statements
that go beyond the evidence, and suggesting alternative expla-
nations for the same observations. Although scientists may dis-
agree about explanations of phenomena, about interpretations
of data, or about the value of rival theories, they do agree that
questioning, response to criticism, and open communication
are integral to the process of science. As scientifi c knowledge
evolves, major disagreements are eventually resolved through
such interactions between scientists. (NSES, 1996, p. 171)
Thus, evaluation is integral to the process of scientifi c
method. One cannot overemphasize the importance of peer-
review to science, and the vigor with which it is carried out.
Full-blown academic battles have been wagged in scientifi c
Fossil Lab at John Day Fossil Beds National Monument. Photo courtesy
of National Park Service.5
The Nature of Science and the Scientifi c Method
Theories are powerful tools (National Science Teachers
Association, The Teaching of Evolution Position Statement):
Scientists seek to develop theories that
• are fi rmly grounded in and based upon evidence;
• are logically consistent with other well-established principles;
• explain more than rival theories; and
• have the potential to lead to new knowledge.
Scientifi c theories are falsifi able and can be reevaluated or
expanded based on new evidence. This is particularly important
in concepts that involve past events, which cannot be tested.
Take, for example, the Big Bang Theory or the Theory of Bio-
logical Evolution as it pertains to the past; both are theories that
explain all of the facts so far gathered from the past, but cannot
be verifi ed as absolute truth, since we cannot go back to test
them. More and more data will be gathered on each to either
support or disprove them. The key force for change in a theory
is, of course, the scientifi c method.
A scientifi c law, said Karl Popper, the famous 20 th century
philosopher, is one that can be proved wrong, like “the sun always
rises in the east.” According to Popper, a law of science can never
be proved; it can only be used to make a prediction that can be
tested, with the possibility of being proved wrong. For example,
as the renowned biologist J.B.S. Haldane replied when asked what
might disprove evolution, “Fossil rabbits in the pre-Cambrian.”
So far that has not happened, and in fact the positive evidence for
the “theory” of evolution is extensive, made up of hundreds of
thousands of mutually corroborating observations. These come
from areas such as geology, paleontology, comparative anatomy,
physiology, biochemistry, ethnology, biogeography, embryology,
and molecular genetics. Like evolution, most accepted scien-
tifi c theories have withstood the test of time and falsifi ability to
become the backbone of further scientifi c investigations.
Science Through the Recent Ages
The term science is relatively modern. Nearly all civiliza-
tions, however, have evidence of methods, concepts, or tech-
niques that were scientifi c in nature. Science has its historical
roots in two primary sources: the technical tradition, in which
practical experiences and skills were passed down and devel-
oped from one generation to another; and the spiritual tradition,
in which human aspirations and ideas were passed on and aug-
mented (Mason, 1962). Observations of the natural world and
their application to daily activities assuredly helped the human
race survive from the earliest times. In western society, it was
not until the Middle Ages, however, that the two converged into
a more pragmatic method that produced results with both tech-
nical and philosophical implications.
An excellent example of the development of science and the
scientifi c method is the demise of the geocentric view of the solar
system. Although it strongly appears to the naked eye that the sun
and moon go around Earth (geocentric), even ancient astral observ-
ers noted that stars moved in a different yearly pattern, and certain
planets or “wanderers” had even stranger movements in the night
sky. In the 16 th and 17 th centuries, observers began to make more
detailed observations of the movements of the stars and planets,
made increasingly complex with the aide of the newly invented
telescope. Galileo improved the telescope enough to observe the
phases of Venus as seen from Earth. With the application of mathe-
matics to their precise measurements, it became obvious to astron-
omers like Copernicus, Kepler, and Galileo that the planets and
Earth must revolve around the sun (heliocentric). It is necessary,
however, to backtrack here a little and make clear that, as early as
the third century B.C., the Greek astronomer Aristarchus proposed
that Earth orbited the sun. Earth’s spherical nature was not only
well known by about 300 B.C., but good measurements of Earth’s
circumference had already been made by that time. Unfortunately,
throughout history, knowledge from one culture has not necessar-
ily been passed on to other cultures or generations.
New discoveries and technological advancements led to
what is known as the Scientifi c Revolution, a period of time
between Copernicus and Sir Isaac Newton during which a core
transformation in “natural philosophy” (science) began in cos-
mology and astronomy and then shifted to physics. Most pro-
foundly, some historians have argued, these changes in thinking
brought important transformations in what came to be held as
“real” and how Europeans justifi ed their claims to knowledge.
The learned view of things in 16th-century thought was that
the world was composed of Four Qualities (Aristotle’s Earth,
Water, Air, and Fire). By contrast, less than two centuries later
Newton’s learned contemporaries believed that the world was
made of atoms or corpuscles (small material bodies). By New-
ton’s day most of learned Europe believed the Earth moved, that
there was no such thing as demonic possession, that claims to
knowledge ... should be based on the authority of our individ-
ual experience, that is, on argument and sensory evidence. The
motto of the Royal Society of London was: Nullius in Verba,
roughly, Accept Nothing on the Basis of Words (or someone
else’s authority). (Hatch, 1991, p. 1)
The Mid-Atlantic Ridge (N is to upper left) on the 2005 Geologic Map of
North America. Location near 50N, 30W.
One of the fi rst to put this idea in print was Rene Descartes.
Although the exact dates of the Scientifi c Revolution may beThe Nature of Science and the Scientifi c Method
6
disputed by science historians, Newton is most commonly con-
sidered the “end” of the revolution, because his work brought
the heavens and Earth together as a universe that operates under
universal laws of motion, changing forever how scientists studied
it. This new world picture, quantitative, logical, comprehensible,
made science a justifi able pursuit, and the study of natural expla-
nations for the world around us grew exponentially. Humans felt
free to not be told how things happen, but to study and detect and
experiment with how the world works in their own ways. Science
has expanded rapidly since the Scientifi c Revolution (Crowe,
1991), and the scientifi c method is well used.
Scientifi c Method and Earth Sciences
Finding fossils in Silurian rocks in Canberra, Australia.
The scientifi c method is not an exact recipe. There are many
ways to apply the scientifi c thought process without necessar-
ily using all the steps listed previously. Even when you encoun-
ter a simple, everyday problem, like the failure of your car to
start when you turn the key in the ignition, you will likely use a
thought process much like the scientifi c method. Your mind will
jump through a succession of hypotheses that you will test until
you fi nd the hypothesis that is correct. For example, you will ask
yourself, is the car out of gas (check gas gauge or remember when
you last fi lled up), is the battery dead (do the lights work?), is
there a short in the ignition apparatus (jiggle the key and the igni-
tion), etc. You will continue thinking of hypotheses and testing
them until you have found one that is correct, and if you don’t,
you will call in an expert who will go through the same process
but with a more educated background in the possible solutions.
Earth science is the study of the physical Earth, from the outer
reaches of the atmosphere to the center of the planet, including all
the interrelationships between atmosphere, water, and rock. This
study is necessary in order to understand the natural world around
us, including natural disasters (from hurricanes to earthquakes to
volcanoes) and where to fi nd and get natural resources (including
energy, minerals, and fresh water) (Punaridge.org, 1998).
As an example of using the scientifi c method, consider a
study of faster fl owing sections of ice that lie within large gla-
ciers in the Antarctic:
1. Research all previous studies in the area and on the topic,
collecting all data, photos, papers, satellite images, etc.,
if there are any.
2. Make fi eld observations of the glacier being studied and
the exceptional “rivers” of ice that fl ow faster than the
ice around them.
3. Identify physical conditions and take measurements
with all necessary technology at your disposal and over
a certain prescribed time frame at the glacier.
4. Construct a model describing a possible method for the
ice in this one section of the glacier to move faster than
the ice around it, as shown by the data collected. One
geologist’s hypothesis was that some liquid material
underlies the area of the glacier in question, providing a
lubricant for the ice.
5. Make predictions based on the model. The prediction
would be that upon drilling to the bottom of the glacier,
a wet material would be found that is not found under
other areas of the glacier.
6. Test the predictions in the fi eld by designing an experiment
to collect the right type of data to answer the questions.
In this case, samples were indeed collected from beneath
specifi c areas of the glacier, a diffi cult and sometimes
dangerous task. Results showed that underlying the faster-
moving areas of ice was a wet mud and gravel slurry not
found in other areas, perhaps from an old stream bed, that
provided lubrication for the ice above it.
Using the scientifi c method can sometimes be complicated
for geologists because Earth is their laboratory and it has many
variables and is NOT a controlled environment. Controlled
experiments (usually carried out in laboratories) are carefully
designed to test a specifi c hypothesis, and they can be repeated.
Unfortunately, many hypotheses in geology cannot be directly
tested in a controlled experiment (e.g., the origin of the Grand
Canyon cannot be discovered by using this approach). Geolo-
gists must collect data by mapping or collecting specimens.
They must rely on circumstantial evidence, which is subject to
interpretation, and therefore can be challenged.
The Theory of Plate Tectonics again is an excellent exam-
ple. Alfred Wegener took some of his own studies and the work
of others and realized that the continents on opposite sides of the
Atlantic Ocean fi t together, and not just in shape, but in geology
and fossil content as well. He proposed a hypothesis that the
continents had drifted apart based on this “circumstantial evi-
dence,” which was not accepted in his lifetime. It took decades
for technology to advance enough for scientists to discover
additional evidence to support his claim that the continents
had once been together (the Atlantic Ocean fl oor was younger
than the continents and had formed between them). As more
and more evidence was produced, his hypothesis was modi-
fi ed and refi ned into a theory we now know as Plate Tectonics.
This theory revolutionized the way humans look at Earth. Many7
The Nature of Science and the Scientifi c Method
On the Nature of Science
1. Science is a way of studying our natural environment,
using a repeatable, methodical approach.
2. Science relies on evidence from the natural world, and
this evidence is examined and interpreted through logic.
3. Science cannot be used, by defi nition, to study events or
phenomena that cannot be perceived by natural or empirical
senses and do not follow any natural rules or regularities.
4. Science is a human endeavor; it is based on observations,
experimentation, and testing. It allows us to connect the
past with the present.
5. Science provides us with a way to present ideas that can
be tested, repeated, and verifi ed.
6. Scientifi c claims are based on testing explanations
against observations of the natural world and rejecting the
ones that fail the test.
7. Scientists gather evidence (as opposed to “proof”) to sup-
port or falsify hypotheses. Hypotheses and theories may
be well supported by evidence but never proven.
8. A scientifi c theory is a well-substantiated explanation for
a set of natural phenomena that has been tested and
verifi ed but is still subject to falsifi cation. Theories are sup-
ported, modifi ed, or replaced as new evidence appears
and are central to scientifi c thinking.
9. There is no such thing as “THE Scientifi c Method.” Scien-
tists in different fi elds often approach their scientifi c test-
ing in different ways.
10. Science is non-dogmatic. Science never requires ideas to
be accepted on belief or faith alone.
11. “Explanations on how the natural world changes based on
myths, personal beliefs, religious values, mystical inspira-
tion, superstition, or authority may be personally useful
and socially relevant, but they are not science.” (NSES,
1996, p. 201)
12. The nature of science “is regarded in contemporary docu-
ments as a fundamental attribute of science literacy and
a defense against unquestioning acceptance of pseudo-
science and of reported research.” (NSTA, 2003. p. 16)
13. Science does not prove nor disprove religious or spiritual
beliefs, nor does it replace either. Science provides a
method of understanding the natural world only.
14. Science cannot make moral or aesthetic judgments.
Understanding how to clone a cat does not indicate
whether cloning is an acceptable endeavor by humans.
Understanding what makes eyes blue or green does not
indicate which is more beautiful.
On Evolution, Creation Science, and
Intelligent Design
1. Creationism, creation science, Intelligent Design (ID), or
any other spiritual concept, involve events or phenomena
that cannot be tested, verifi ed, or repeated through scien-
tifi c methodology and, therefore, cannot be measured using
scientifi c practice. Because science is limited to explaining
natural phenomena through the use of empirical evidence,
it cannot provide religious or ultimate explanations.
2. Evolution is a theory greatly accepted by the scientifi c
community because all available evidence supports the
central conclusions of evolutionary theory, that life on
Earth has evolved and that species share common ances-
tors and genomes.
3. Vigorous questioning of existing ideas is central to the
scientifi c process. Solid and long-held theories such as
evolution or relativity stand as important foundations of
science because they have proven, so far, unassailable
(but not from want of trying...).
4. Evolution is a theory that has developed since Darwin’s
initial concepts. It is not a static idea, but a growing
concept added to by scientifi c observation, testing, and
debate.
5. Science teachers should not advocate any religious inter-
pretations of nature and should be nonjudgmental about
the personal beliefs of students. (NSTA recommendation)
6. “Do you believe in evolution?” The answer might be,
“Believe is not the appropriate term, since it implies faith
not based on evidence. I accept the inference that Earth
is very old and life has changed over billions of years
because that is what the evidence tells us.” Science is not
about belief—it is about making inferences based on evi-
dence, and there is overwhelming evidence for evolution
from many different disciplines. (Adapted from the Under-
standing Evolution Web site.)The Nature of Science and the Scientifi c Method
unexplained geologic phenomenon now make perfect sense in
the light of Plate Tectonics.
Other Earth science–related discoveries that caused major
conceptual changes in the way humans view their world were
the discovery that Earth is spherical and not fl at; that all the
planets revolve around the sun, not around Earth; and that fos-
sils give us a detailed, logical record of the evolutionary devel-
opment of biological organisms on Earth. Today, incredible
discoveries are being made in the fi eld of astronomy, all based
again on circumstantial evidence and observation with increas-
ingly more powerful and varied telescopes.
Conclusion
Percy W. Bridgman, author of Refl ections of a Physicist in
1955 and winner of the 1946 Nobel Prize in physics, perhaps
most clearly states in “On Scientifi c Method” how the use of
the scientifi c method by scientists does not often follow a set
formula or recipe, nor should it, since that may stifl e human
innovation and creativity, often necessary in producing new and
revolutionary hypotheses:
Scientifi c method is what working scientists do, not what other
people or even they themselves may say about it. No working
scientist, when he plans an experiment in the laboratory, asks
himself whether he is being properly scientifi c, nor is he inter-
ested in whatever method he may be using as method. When the
scientist ventures to criticize the work of his fellow scientist, as
is not uncommon, he does not base his criticism on such glitter-
ing generalities as failure to follow the “scientifi c method,” but
his criticism is specifi c, based on some feature characteristic
of the particular situation. The working scientist is always too
much concerned with getting down to brass tacks to be willing
to spend his time on generalities.
But to the working scientist himself all this [the steps of sci-
entifi c method] appears obvious and trite. What appears to
him as the essence of the situation is that he is not consciously
following any prescribed course of action, but feels complete
freedom to utilize any method or device whatever, which in the
particular situation before him seems likely to yield the correct
answer. In his attack on his specifi c problem he suffers no inhi-
bitions of precedent or authority, but is completely free to adopt
any course that his ingenuity is capable of suggesting to him.
8
No one standing on the outside can predict what the individual
scientist will do or what method he will follow. In short, science
is what scientists do, and there are as many scientifi c methods
as there are individual scientists.
"They never asked for a nickel from us in return," said the KU researcher. "There was an Islamist attack on a gas facility
at the same time near the Algerian-Libyan border, and they killed 30-40 workers. So the security protected us and potentially saved our lives."














